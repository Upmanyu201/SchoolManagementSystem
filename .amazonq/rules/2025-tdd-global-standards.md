# ðŸ§ª TDD Rules (2025 Global Standards)

## Core TDD Principles (Red-Green-Refactor)

### 1. Red Phase - Write Failing Test First
```python
# Always start with a failing test
@pytest.mark.asyncio
async def test_student_enrollment_should_create_student():
    # Arrange
    student_data = {
        'name': 'John Doe',
        'admission_number': 'ADM001',
        'class_id': 1
    }
    
    # Act & Assert - This should fail initially
    student = await StudentService.enroll_student(student_data)
    assert student.name == 'John Doe'
    assert student.admission_number == 'ADM001'
```

### 2. Green Phase - Write Minimal Code
```python
# Write just enough code to pass the test
class StudentService:
    @staticmethod
    async def enroll_student(data: dict) -> Student:
        return await Student.objects.acreate(**data)
```

### 3. Refactor Phase - Improve Code Quality
```python
# Refactor for better design
class StudentService:
    @staticmethod
    async def enroll_student(data: dict, user: User) -> Student:
        # Validate permissions
        if not user.has_perm('students.add_student'):
            raise PermissionError("Insufficient permissions")
        
        # Validate data
        validated_data = await StudentValidator.validate(data)
        
        # Create student
        student = await Student.objects.acreate(**validated_data)
        
        # Initialize related data
        await FeeService.initialize_fees(student)
        
        return student
```

## 2025 TDD Patterns

### AI-Enhanced Testing
```python
# Property-based testing with AI
from hypothesis import given, strategies as st
from hypothesis_auto import auto_pytest_magic

@given(auto_pytest_magic(StudentCreateSchema))
async def test_student_creation_properties(student_data):
    """AI generates test cases automatically"""
    student = await StudentService.create(student_data)
    assert student.is_valid()

# AI test generation
@pytest.mark.ai_generated
async def test_edge_cases():
    """Generated by AI based on code analysis"""
    pass
```

### Async-First Testing
```python
# All tests are async by default
@pytest.mark.asyncio
async def test_concurrent_fee_payments():
    student = await StudentFactory.acreate()
    
    # Test concurrent operations
    tasks = [
        FeeService.process_payment(student.id, 1000),
        FeeService.process_payment(student.id, 500)
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    assert len([r for r in results if not isinstance(r, Exception)]) == 1
```

### Behavior-Driven Development (BDD)
```python
# Gherkin-style tests
@pytest.mark.bdd
async def test_student_fee_payment_workflow():
    """
    Given a student with pending fees
    When payment is processed
    Then fee status should be updated
    And receipt should be generated
    """
    # Given
    student = await StudentFactory.acreate()
    await FeeFactory.acreate(student=student, amount=1000, status='pending')
    
    # When
    receipt = await FeeService.process_payment(student.id, 1000)
    
    # Then
    fee = await Fee.objects.aget(student=student)
    assert fee.status == 'paid'
    assert receipt.amount == 1000
```

### Contract Testing
```python
# API contract testing
@pytest.mark.contract
async def test_student_api_contract():
    """Ensure API contracts are maintained"""
    response = await client.post('/api/students/', {
        'name': 'Test Student',
        'admission_number': 'TEST001'
    })
    
    # Validate response schema
    assert response.status_code == 201
    assert 'id' in response.json()
    assert 'created_at' in response.json()
```

### Performance Testing in TDD
```python
# Performance requirements as tests
@pytest.mark.performance
async def test_student_search_performance():
    """Search should complete within 50ms"""
    # Create test data
    await StudentFactory.acreate_batch(1000)
    
    start_time = time.time()
    results = await StudentService.search('John')
    duration = time.time() - start_time
    
    assert duration < 0.05  # 50ms requirement
    assert len(results) > 0
```

### ML-Driven Testing (2025)
```python
# ML model testing
@pytest.mark.ml
async def test_student_performance_prediction_model():
    """Test ML model accuracy and reliability"""
    from sklearn.metrics import accuracy_score
    
    # Arrange - Create training data
    students = await StudentFactory.acreate_batch(100)
    features = await MLService.extract_features(students)
    labels = await MLService.get_performance_labels(students)
    
    # Act - Train and predict
    model = await MLService.train_performance_model(features, labels)
    predictions = await model.predict(features)
    
    # Assert - Model performance
    accuracy = accuracy_score(labels, predictions)
    assert accuracy > 0.85  # 85% minimum accuracy
    assert model.feature_importance_ is not None

# ML data drift testing
@pytest.mark.ml_monitoring
async def test_data_drift_detection():
    """Detect if input data distribution has changed"""
    from scipy.stats import ks_2samp
    
    # Historical data
    historical_features = await MLService.get_historical_features()
    current_features = await MLService.get_current_features()
    
    # Statistical test for drift
    for feature_name in historical_features.columns:
        statistic, p_value = ks_2samp(
            historical_features[feature_name],
            current_features[feature_name]
        )
        
        # Assert no significant drift (p > 0.05)
        assert p_value > 0.05, f"Data drift detected in {feature_name}"

# ML model bias testing
@pytest.mark.ml_fairness
async def test_model_fairness_across_demographics():
    """Ensure ML model is fair across different student groups"""
    students = await StudentFactory.acreate_batch(200)
    
    # Test predictions across gender groups
    male_students = [s for s in students if s.gender == 'M']
    female_students = [s for s in students if s.gender == 'F']
    
    male_predictions = await MLService.predict_performance(male_students)
    female_predictions = await MLService.predict_performance(female_students)
    
    # Statistical parity test
    male_avg = sum(male_predictions) / len(male_predictions)
    female_avg = sum(female_predictions) / len(female_predictions)
    
    # Assert no significant bias (difference < 5%)
    bias_ratio = abs(male_avg - female_avg) / max(male_avg, female_avg)
    assert bias_ratio < 0.05, f"Model bias detected: {bias_ratio:.3f}"

# ML feature importance testing
@pytest.mark.ml_explainability
async def test_model_feature_importance():
    """Validate that important features make sense"""
    model = await MLService.get_trained_model()
    feature_names = await MLService.get_feature_names()
    
    # Get feature importance
    importance_dict = dict(zip(feature_names, model.feature_importances_))
    
    # Assert expected important features
    assert importance_dict['attendance_rate'] > 0.1
    assert importance_dict['previous_grades'] > 0.15
    assert importance_dict['assignment_completion'] > 0.1
    
    # Assert irrelevant features have low importance
    assert importance_dict.get('student_id', 0) < 0.01

# ML A/B testing
@pytest.mark.ml_ab_test
async def test_ml_model_ab_comparison():
    """Compare new ML model against baseline"""
    test_students = await StudentFactory.acreate_batch(50)
    
    # Get predictions from both models
    baseline_predictions = await MLService.predict_with_baseline(test_students)
    new_model_predictions = await MLService.predict_with_new_model(test_students)
    
    # Get actual outcomes
    actual_outcomes = await MLService.get_actual_outcomes(test_students)
    
    # Compare model performance
    baseline_accuracy = accuracy_score(actual_outcomes, baseline_predictions)
    new_model_accuracy = accuracy_score(actual_outcomes, new_model_predictions)
    
    # Assert new model is better or at least not worse
    assert new_model_accuracy >= baseline_accuracy - 0.02  # Allow 2% tolerance

# ML pipeline testing
@pytest.mark.ml_pipeline
async def test_ml_training_pipeline():
    """Test complete ML training pipeline"""
    # Test data preparation
    raw_data = await MLService.get_raw_student_data()
    processed_data = await MLService.preprocess_data(raw_data)
    
    assert processed_data.shape[0] > 0
    assert not processed_data.isnull().any().any()
    
    # Test feature engineering
    features = await MLService.engineer_features(processed_data)
    assert 'attendance_rate' in features.columns
    assert 'grade_trend' in features.columns
    
    # Test model training
    model = await MLService.train_model(features)
    assert hasattr(model, 'predict')
    assert hasattr(model, 'feature_importances_')

# ML inference testing
@pytest.mark.ml_inference
async def test_ml_real_time_inference():
    """Test ML model inference in production-like conditions"""
    student = await StudentFactory.acreate()
    
    # Test single prediction
    start_time = time.time()
    prediction = await MLService.predict_single_student(student.id)
    inference_time = time.time() - start_time
    
    # Assert prediction quality and speed
    assert 0 <= prediction <= 1  # Probability score
    assert inference_time < 0.1  # 100ms max inference time
    
    # Test batch prediction
    students = await StudentFactory.acreate_batch(10)
    student_ids = [s.id for s in students]
    
    start_time = time.time()
    predictions = await MLService.predict_batch(student_ids)
    batch_time = time.time() - start_time
    
    assert len(predictions) == len(student_ids)
    assert batch_time < 0.5  # 500ms max for batch of 10
```

## Test Organization (2025 Standards)

### Test Structure
```
dev_tools/testing/
â”œâ”€â”€ unit/                    # Unit tests (60%)
â”‚   â”œâ”€â”€ students/
â”‚   â”œâ”€â”€ fees/
â”‚   â””â”€â”€ reports/
â”œâ”€â”€ integration/             # Integration tests (20%)
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ database/
â”‚   â””â”€â”€ services/
â”œâ”€â”€ e2e/                     # End-to-end tests (10%)
â”‚   â”œâ”€â”€ playwright/
â”‚   â””â”€â”€ scenarios/
â”œâ”€â”€ ml/                      # ML tests (10%)
â”‚   â”œâ”€â”€ model_validation/
â”‚   â”œâ”€â”€ data_quality/
â”‚   â”œâ”€â”€ fairness/
â”‚   â”œâ”€â”€ performance/
â”‚   â””â”€â”€ pipeline/
â”œâ”€â”€ performance/             # Performance tests
â”œâ”€â”€ security/                # Security tests
â”œâ”€â”€ fixtures/                # Test data
â””â”€â”€ utils/                   # Test utilities
```

### Test Naming Convention
```python
# Pattern: test_[unit_of_work]_[scenario]_[expected_behavior]
async def test_student_enrollment_with_valid_data_should_create_student():
    pass

async def test_student_enrollment_with_duplicate_admission_number_should_raise_error():
    pass

async def test_fee_payment_with_insufficient_amount_should_create_partial_payment():
    pass
```

### Factory Pattern (2025)
```python
# Modern factory with async support
import factory
from factory import fuzzy

class StudentFactory(factory.django.DjangoModelFactory):
    class Meta:
        model = Student
        django_get_or_create = ('admission_number',)
    
    name = factory.Faker('name')
    admission_number = factory.Sequence(lambda n: f'ADM{n:04d}')
    email = factory.LazyAttribute(lambda obj: f'{obj.name.lower().replace(" ", ".")}@school.edu')
    date_of_birth = fuzzy.FuzzyDate(date(2005, 1, 1), date(2015, 12, 31))
    
    # Async factory methods
    @classmethod
    async def acreate(cls, **kwargs):
        return await sync_to_async(cls.create)(**kwargs)
    
    @classmethod
    async def acreate_batch(cls, size, **kwargs):
        return await sync_to_async(cls.create_batch)(size, **kwargs)
```

### Mock Patterns (2025)
```python
# Async mocking
from unittest.mock import AsyncMock, patch

@patch('services.email_service.send_email')
async def test_fee_reminder_sends_email(mock_send_email):
    mock_send_email.return_value = AsyncMock()
    
    student = await StudentFactory.acreate()
    await FeeService.send_reminder(student.id)
    
    mock_send_email.assert_called_once_with(
        to=student.email,
        template='fee_reminder',
        context={'student': student}
    )
```

## Test Configuration (2025)

### pytest.ini
```ini
[tool:pytest]
DJANGO_SETTINGS_MODULE = config.settings.test
addopts = 
    --strict-markers
    --strict-config
    --cov=.
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=90
    --asyncio-mode=auto
    --tb=short
markers =
    asyncio: async tests
    unit: unit tests
    integration: integration tests
    e2e: end-to-end tests
    performance: performance tests
    security: security tests
    ai_generated: AI generated tests
    bdd: behavior driven tests
    contract: contract tests
    ml: machine learning tests
    ml_monitoring: ML monitoring tests
    ml_fairness: ML fairness tests
    ml_explainability: ML explainability tests
    ml_ab_test: ML A/B testing
    ml_pipeline: ML pipeline tests
    ml_inference: ML inference tests
```

### Test Settings
```python
# config/settings/test.py
from .base import *

# Fast test database
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': ':memory:',
    }
}

# Disable migrations for speed
class DisableMigrations:
    def __contains__(self, item): return True
    def __getitem__(self, item): return None

MIGRATION_MODULES = DisableMigrations()

# Fast password hashing
PASSWORD_HASHERS = ['django.contrib.auth.hashers.MD5PasswordHasher']

# Disable logging during tests
LOGGING_CONFIG = None

# Test-specific settings
EMAIL_BACKEND = 'django.core.mail.backends.locmem.EmailBackend'
CELERY_TASK_ALWAYS_EAGER = True
CACHES = {'default': {'BACKEND': 'django.core.cache.backends.dummy.DummyCache'}}
```

## CI/CD Integration (2025)

### GitHub Actions Workflow
```yaml
# .github/workflows/tdd.yml
name: TDD Pipeline
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install UV
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
    
    - name: Install dependencies
      run: uv pip sync requirements-test.txt
    
    - name: Run unit tests
      run: pytest dev_tools/testing/unit/ -v --cov=.
    
    - name: Run integration tests
      run: pytest dev_tools/testing/integration/ -v
    
    - name: Run E2E tests
      run: playwright test
    
    - name: Security tests
      run: bandit -r . -f json -o security-report.json
    
    - name: Performance tests
      run: pytest dev_tools/testing/performance/ -v
    
    - name: ML tests
      run: pytest dev_tools/testing/ml/ -v
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

## Advanced TDD Patterns (2025)

### Mutation Testing
```python
# Test the tests themselves
# pip install mutmut
# mutmut run --paths-to-mutate=students/
async def test_student_validation_catches_all_errors():
    """Ensure validation is comprehensive"""
    invalid_data = {'name': '', 'admission_number': None}
    
    with pytest.raises(ValidationError) as exc_info:
        await StudentService.create(invalid_data)
    
    errors = exc_info.value.message_dict
    assert 'name' in errors
    assert 'admission_number' in errors
```

### Snapshot Testing
```python
# Visual regression testing
@pytest.mark.snapshot
async def test_student_report_layout():
    """Ensure report layout doesn't change unexpectedly"""
    student = await StudentFactory.acreate()
    report_html = await ReportService.generate_student_report(student.id)
    
    # Compare with stored snapshot
    assert report_html == snapshot('student_report.html')
```

### Chaos Engineering Tests
```python
# Test system resilience
@pytest.mark.chaos
async def test_system_handles_database_failure():
    """System should gracefully handle DB failures"""
    with patch('django.db.connection.cursor') as mock_cursor:
        mock_cursor.side_effect = DatabaseError("Connection lost")
        
        response = await client.get('/api/students/')
        assert response.status_code == 503
        assert 'temporarily unavailable' in response.json()['message']
```

## TDD Metrics & Monitoring

### Coverage Requirements
```python
# Minimum coverage thresholds
COVERAGE_REQUIREMENTS = {
    'unit_tests': 90,      # 90% unit test coverage
    'integration': 80,     # 80% integration coverage
    'e2e': 70,            # 70% E2E coverage
    'ml_tests': 85,       # 85% ML model coverage
    'mutation_score': 85   # 85% mutation test score
}

# ML-specific metrics
ML_QUALITY_REQUIREMENTS = {
    'model_accuracy': 0.85,     # Minimum 85% accuracy
    'inference_time': 0.1,      # Max 100ms inference
    'data_drift_threshold': 0.05, # 5% drift tolerance
    'bias_threshold': 0.05,     # 5% bias tolerance
    'feature_stability': 0.9    # 90% feature stability
}
```

### Test Performance Monitoring
```python
# Track test execution time
@pytest.fixture(autouse=True)
def track_test_performance(request):
    start_time = time.time()
    yield
    duration = time.time() - start_time
    
    # Log slow tests
    if duration > 1.0:  # 1 second threshold
        logger.warning(f"Slow test: {request.node.name} took {duration:.2f}s")
```

## Quick TDD Checklist (2025)

- [ ] Write failing test first (Red)
- [ ] Write minimal code to pass (Green)
- [ ] Refactor for quality (Refactor)
- [ ] Use async/await for I/O operations
- [ ] Include property-based testing
- [ ] Add performance assertions
- [ ] Mock external dependencies
- [ ] Test error conditions
- [ ] Validate API contracts
- [ ] Include security tests
- [ ] **Test ML models for accuracy & bias**
- [ ] **Monitor ML data drift**
- [ ] **Validate ML feature importance**
- [ ] **Test ML inference performance**
- [ ] **A/B test ML model versions**
- [ ] Maintain 90%+ coverage
- [ ] Run tests in CI/CD pipeline
- [ ] Use factories for test data
- [ ] Follow BDD scenarios
- [ ] Monitor test performance

## Global Industry Adoption

### India ðŸ‡®ðŸ‡³
- Focus on cost-effective testing
- Heavy use of open-source ML tools (scikit-learn, TensorFlow)
- Emphasis on API and ML model testing
- AutoML testing frameworks

### China ðŸ‡¨ðŸ‡³
- AI-powered test generation with ML
- Microservices + ML pipeline testing
- Performance-first ML inference testing
- Large-scale ML model validation

### USA ðŸ‡ºðŸ‡¸
- Cloud-native ML testing (AWS SageMaker, Azure ML)
- MLOps integration with DevOps
- Security-focused ML testing
- Responsible AI testing frameworks

### Japan ðŸ‡¯ðŸ‡µ
- Quality-first ML methodology
- Detailed ML model documentation
- Continuous ML improvement (Kaizen)
- Precision-focused ML testing

### Europe ðŸ‡ªðŸ‡º
- GDPR compliance for ML models
- AI ethics and fairness testing
- Explainable AI testing
- Sustainability metrics for ML training

This TDD approach ensures high-quality, maintainable code with robust ML capabilities while following global 2025 standards for modern web development and AI/ML systems.

## ML Testing Tools & Frameworks (2025)

### Essential ML Testing Libraries
```python
# requirements-ml-test.txt
pytest-ml==2.1.0
hypothesis==6.92.0
great-expectations==0.18.0
evidently==0.4.0
alibi-detect==0.11.0
fairlearn==0.9.0
shap==0.42.0
scikit-learn==1.3.0
tensorflow==2.14.0
torch==2.1.0
mlflow==2.8.0
wandb==0.16.0
```

### ML Test Configuration
```python
# conftest.py for ML tests
import pytest
import numpy as np
from sklearn.datasets import make_classification

@pytest.fixture
def sample_ml_data():
    """Generate sample data for ML testing"""
    X, y = make_classification(
        n_samples=1000,
        n_features=10,
        n_informative=5,
        n_redundant=2,
        random_state=42
    )
    return X, y

@pytest.fixture
def trained_model(sample_ml_data):
    """Provide a trained model for testing"""
    from sklearn.ensemble import RandomForestClassifier
    X, y = sample_ml_data
    model = RandomForestClassifier(random_state=42)
    model.fit(X, y)
    return model
```